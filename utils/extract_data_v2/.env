# ===================
# CONFIGURACIÓN AWS
# ===================
AWS_PROFILE=prod-compliance-admin
REGION=us-east-1

# ===================
# CONFIGURACIÓN PRINCIPAL
# ===================
PROJECT_NAME=datalake
TEAM=apdayc
DATA_SOURCE=bigmagic
ENDPOINT_NAME=PEBDDATA2
ENVIRONMENT=DEV

# ===================
# ALMACENAMIENTO
# ===================
S3_RAW_BUCKET=sofia-dev-datalake-510543735161-us-east-1-raw-s3
DYNAMO_LOGS_TABLE=sofia-dev-datalake-logs-ddb
TOPIC_ARN=arn:aws:sns:us-east-1:510543735161:sofia-dev-datalake-failed-sns

# ===================
# ARCHIVOS DE CONFIGURACIÓN
# ===================
TABLES_CSV_S3=./config/tables.csv
CREDENTIALS_CSV_S3=./config/credentials.csv
COLUMNS_CSV_S3=./config/columns.csv

# ===================
# CONFIGURACIÓN DE PROCESAMIENTO OPTIMIZADA
# ===================
# Reducir threads para evitar sobrecarga de conexiones
MAX_THREADS=2

# Reducir chunk size para evitar timeouts
CHUNK_SIZE=50000

# Formato de salida
OUTPUT_FORMAT=parquet
EXTRACTOR_TYPE=sqlserver
LOADER_TYPE=s3
MONITOR_TYPE=dynamodb

# ===================
# CONFIGURACIÓN ADICIONAL PARA RESOLVER ERRORES
# ===================
# Timeout de conexión en segundos (15 minutos)
CONNECTION_TIMEOUT=900

# Timeout de login en segundos (5 minutos)  
LOGIN_TIMEOUT=300

# Número máximo de reintentos
MAX_RETRIES=3

# Delay entre reintentos (segundos)
RETRY_DELAY=10

# Usar SQLAlchemy engine en lugar de pymssql directo
USE_SQLALCHEMY=true

# Pool size para conexiones
CONNECTION_POOL_SIZE=5

# Recycle connections después de 1 hora
CONNECTION_POOL_RECYCLE=3600