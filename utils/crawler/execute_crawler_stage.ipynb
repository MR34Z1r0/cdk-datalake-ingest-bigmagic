{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb951642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-12 10:14:47 - CrawlerStage - INFO - [<module>:96] - Iniciando CrawlerStage...\n",
      "2025-06-12 10:14:47 - CrawlerStage - INFO - [validate_arguments:65] - Validaci√≥n de argumentos completada exitosamente\n",
      "2025-06-12 10:14:47 - CrawlerStage - INFO - [<module>:121] - Inicializando clientes AWS...\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:128] - Clientes AWS inicializados correctamente\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:139] - Tabla de configuraci√≥n DynamoDB: sofia-prod-datalake-configuration-ddb\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:140] - Tabla de endpoints DynamoDB: sofia-prod-datalake-credentials-ddb\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:145] - Conexiones a tablas DynamoDB establecidas\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:155] - Configuraci√≥n establecida - Endpoint: PEBDDATA2, S3 Target: s3://sofia-566121885938-us-west-2-prod-datalake-stage-s3/\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:159] - Obteniendo datos del endpoint: PEBDDATA2\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:166] - Datos del endpoint obtenidos exitosamente: mssql\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:175] - Nombres generados - Database: sofia_apdayc_pebddata2_stage, Crawler: sofia_apdayc_pebddata2_stage_crawler\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [<module>:1074] - Iniciando verificaciones de salud del sistema...\n",
      "2025-06-12 10:14:49 - CrawlerStage - INFO - [health_check:822] - Ejecutando verificaciones de salud...\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:930] - Resumen de verificaciones de salud:\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:931] - ------------------------------------------------------------\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:938] - ‚úÖ AWS Glue                  - Conectividad OK\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:938] - ‚úÖ AWS Lake Formation        - Conectividad OK\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:938] - ‚úÖ DynamoDB Config Table     - Estado: ACTIVE\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:938] - ‚úÖ DynamoDB Endpoint Table   - Estado: ACTIVE\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:938] - ‚úÖ IAM Permissions           - Principal: arn:aws:iam::566121885938:user/miguel.espinoza\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:938] - ‚úÖ S3 Access                 - Bucket accesible: sofia-566121885938-us-west-2-prod-datalake-stage-s3\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:938] - ‚úÖ Endpoint Data             - Endpoint 'PEBDDATA2' encontrado\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:945] - ------------------------------------------------------------\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [health_check:948] - üéâ Todas las verificaciones de salud pasaron exitosamente\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [main:670] - ============================================================\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [main:671] - INICIANDO PROCESO CRAWLER STAGE\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [main:672] - ============================================================\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [main:676] - Paso 1: Obteniendo estado de las tablas...\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [get_dynamo_crawler_status_for_endpoint:600] - Obteniendo estado del crawler para endpoint: PEBDDATA2\n",
      "2025-06-12 10:14:54 - CrawlerStage - INFO - [get_dynamo_crawler_status_for_endpoint:609] - Escaneando tabla de configuraci√≥n...\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [get_dynamo_crawler_status_for_endpoint:612] - Se encontraron 84 elementos en la tabla de configuraci√≥n\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [get_dynamo_crawler_status_for_endpoint:645] - Procesamiento completado - Total: 84, Sin crawler: 0, Errores: 0\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [get_dynamo_crawler_status_for_endpoint:657] - Se actualizaron 0 de 0 tablas en DynamoDB\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [main:684] - Tablas encontradas: 84, Nuevas tablas: 0\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [main:687] - Paso 2: Verificando existencia del crawler...\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [get_crawler:511] - Verificando existencia del crawler: sofia_apdayc_pebddata2_stage_crawler\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [get_crawler:522] - Crawler 'sofia_apdayc_pebddata2_stage_crawler' no existe\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [main:710] - El crawler no existe, procediendo a crearlo...\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [main:713] - Paso 3: Verificando base de datos del cat√°logo...\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [get_database_data_catalog:208] - Verificando existencia de base de datos: sofia_apdayc_pebddata2_stage\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [get_database_data_catalog:212] - Base de datos 'sofia_apdayc_pebddata2_stage' encontrada\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [main:717] - La base de datos ya existe\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [main:720] - Paso 4: Creando nuevo crawler...\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [create_crawler:434] - Creando crawler: sofia_apdayc_pebddata2_stage_crawler\n",
      "2025-06-12 10:14:59 - CrawlerStage - INFO - [build_crawler_targets:389] - Construyendo targets para 84 tablas\n",
      "2025-06-12 10:15:32 - CrawlerStage - INFO - [build_crawler_targets:428] - Se construyeron 84 targets exitosamente\n",
      "2025-06-12 10:15:32 - CrawlerStage - INFO - [create_crawler:459] - Crawler 'sofia_apdayc_pebddata2_stage_crawler' creado exitosamente\n",
      "2025-06-12 10:15:32 - CrawlerStage - INFO - [main:722] - Crawler creado exitosamente\n",
      "2025-06-12 10:15:32 - CrawlerStage - INFO - [main:724] - Paso 5: Iniciando nuevo crawler...\n",
      "2025-06-12 10:15:32 - CrawlerStage - INFO - [start_crawler:534] - Iniciando crawler: sofia_apdayc_pebddata2_stage_crawler\n",
      "2025-06-12 10:15:32 - CrawlerStage - INFO - [start_crawler:545] - Estado del crawler: READY, procediendo a iniciar...\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [start_crawler:551] - Crawler 'sofia_apdayc_pebddata2_stage_crawler' iniciado exitosamente\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:726] - Crawler iniciado exitosamente\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:789] - ============================================================\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:790] - PROCESO COMPLETADO EXITOSAMENTE\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:791] - Tiempo de ejecuci√≥n: 44.17 segundos\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:792] - Crawler: sofia_apdayc_pebddata2_stage_crawler\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:793] - Base de datos: sofia_apdayc_pebddata2_stage\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:794] - Tablas procesadas: 84\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:795] - Nuevas tablas agregadas: 0\n",
      "2025-06-12 10:15:38 - CrawlerStage - INFO - [main:796] - ============================================================\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import boto3\n",
    "import pytz\n",
    "#from awsglue.utils import getResolvedOptions\n",
    "from botocore.exceptions import ClientError, BotoCoreError\n",
    "\n",
    "boto3.setup_default_session(profile_name='prd-valorx-admin', region_name='us-west-2')\n",
    "\n",
    "# Configuraci√≥n de logging mejorada\n",
    "def setup_logging():\n",
    "    \"\"\"Configura el sistema de logging con formato detallado y manejo de errores\"\"\"\n",
    "    log_level = os.environ.get(\"LOGGING\", \"INFO\").upper()\n",
    "    \n",
    "    # Crear formateador personalizado\n",
    "    formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Configurar handler para consola\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Configurar logger principal\n",
    "    logger = logging.getLogger(\"CrawlerStage\")\n",
    "    logger.setLevel(getattr(logging, log_level, logging.INFO))\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # Evitar duplicaci√≥n de logs\n",
    "    logger.propagate = False\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# Constantes de configuraci√≥n\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5  # segundos\n",
    "TIMEOUT_SECONDS = 300  # 5 minutos\n",
    "\n",
    "class CrawlerStageError(Exception):\n",
    "    \"\"\"Excepci√≥n personalizada para errores del CrawlerStage\"\"\"\n",
    "    pass\n",
    "\n",
    "def validate_arguments(args: Dict[str, str]) -> None:\n",
    "    \"\"\"Valida que todos los argumentos requeridos est√©n presentes\"\"\"\n",
    "    required_args = [\n",
    "        'JOB_NAME', 'S3_STAGE_PREFIX', 'DYNAMO_CONFIG_TABLE', \n",
    "        'DYNAMO_ENDPOINT_TABLE', 'ENDPOINT', 'PROCESS_ID', \n",
    "        'ARN_ROLE_CRAWLER', 'PROJECT_NAME', 'TEAM', 'DATA_SOURCE'\n",
    "    ]\n",
    "    \n",
    "    missing_args = [arg for arg in required_args if not args.get(arg)]\n",
    "    if missing_args:\n",
    "        raise CrawlerStageError(f\"Argumentos faltantes: {', '.join(missing_args)}\")\n",
    "    \n",
    "    logger.info(f\"Validaci√≥n de argumentos completada exitosamente\")\n",
    "    logger.debug(f\"Argumentos recibidos: {list(args.keys())}\")\n",
    "\n",
    "def retry_on_failure(max_retries: int = MAX_RETRIES, delay: int = RETRY_DELAY):\n",
    "    \"\"\"Decorador para reintentar operaciones que fallan\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    if attempt > 0:\n",
    "                        logger.info(f\"Reintento {attempt}/{max_retries} para {func.__name__}\")\n",
    "                        time.sleep(delay * attempt)  # Backoff exponencial\n",
    "                    \n",
    "                    return func(*args, **kwargs)\n",
    "                    \n",
    "                except (ClientError, BotoCoreError, Exception) as e:\n",
    "                    last_exception = e\n",
    "                    logger.warning(f\"Intento {attempt + 1} fall√≥ para {func.__name__}: {str(e)}\")\n",
    "                    \n",
    "                    if attempt == max_retries:\n",
    "                        logger.error(f\"Todos los reintentos fallaron para {func.__name__}\")\n",
    "                        break\n",
    "            \n",
    "            raise last_exception\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Inicializaci√≥n con validaci√≥n\n",
    "try:\n",
    "    logger.info(\"Iniciando CrawlerStage...\")\n",
    "    \n",
    "    # @params: [JOB_NAME]\n",
    "    #args = getResolvedOptions(\n",
    "    #    sys.argv, ['JOB_NAME', 'S3_STAGE_PREFIX', 'DYNAMO_CONFIG_TABLE', \n",
    "    #              'DYNAMO_ENDPOINT_TABLE', 'ENDPOINT', 'PROCESS_ID', \n",
    "    #              'ARN_ROLE_CRAWLER', 'PROJECT_NAME', 'TEAM', 'DATA_SOURCE']\n",
    "    #)\n",
    "    \n",
    "    args = {\n",
    "        'JOB_NAME': 'sofia-prod-datalake-apdayc_crawler_stage-job',\n",
    "        'S3_STAGE_PREFIX': 's3://sofia-566121885938-us-west-2-prod-datalake-stage-s3/',\n",
    "        'DYNAMO_CONFIG_TABLE': 'sofia-prod-datalake-configuration-ddb',\n",
    "        'DYNAMO_ENDPOINT_TABLE': 'sofia-prod-datalake-credentials-ddb',\n",
    "        'ENDPOINT': 'PEBDDATA2',\n",
    "        'PROCESS_ID': '10',\n",
    "        'ARN_ROLE_CRAWLER': 'arn:aws:iam::566121885938:role/sofia-prod-datalake-apdayc_crawler_stage-role',\n",
    "        'PROJECT_NAME': 'datalake',\n",
    "        'TEAM': 'sofia',\n",
    "        'DATA_SOURCE': 'apdayc'\n",
    "    }\n",
    "\n",
    "    validate_arguments(args)\n",
    "    \n",
    "    # Inicializaci√≥n de clientes AWS con manejo de errores\n",
    "    logger.info(\"Inicializando clientes AWS...\")\n",
    "    \n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    dynamodb_client = boto3.client('dynamodb')  # Cliente adicional para describe_table\n",
    "    client_glue = boto3.client('glue')\n",
    "    client_lakeformation = boto3.client('lakeformation')\n",
    "    \n",
    "    logger.info(\"Clientes AWS inicializados correctamente\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error cr√≠tico durante la inicializaci√≥n: {str(e)}\")\n",
    "    logger.critical(f\"Traceback completo: {traceback.format_exc()}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Variables globales con logging\n",
    "dynamo_config_table = args['DYNAMO_CONFIG_TABLE']\n",
    "dynamo_endpoint_table = args['DYNAMO_ENDPOINT_TABLE']\n",
    "\n",
    "logger.info(f\"Tabla de configuraci√≥n DynamoDB: {dynamo_config_table}\")\n",
    "logger.info(f\"Tabla de endpoints DynamoDB: {dynamo_endpoint_table}\")\n",
    "\n",
    "try:\n",
    "    config_table_metadata = dynamodb.Table(dynamo_config_table)\n",
    "    endpoint_table_metadata = dynamodb.Table(dynamo_endpoint_table)\n",
    "    logger.info(\"Conexiones a tablas DynamoDB establecidas\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error conectando a tablas DynamoDB: {str(e)}\")\n",
    "    raise CrawlerStageError(f\"No se pudo conectar a las tablas DynamoDB: {str(e)}\")\n",
    "\n",
    "s3_target = args['S3_STAGE_PREFIX']\n",
    "arn_role_crawler = args['ARN_ROLE_CRAWLER']\n",
    "job_name = args['JOB_NAME']\n",
    "endpoint_name = args['ENDPOINT']\n",
    "\n",
    "logger.info(f\"Configuraci√≥n establecida - Endpoint: {endpoint_name}, S3 Target: {s3_target}\")\n",
    "\n",
    "# Obtener datos del endpoint con validaci√≥n\n",
    "try:\n",
    "    logger.info(f\"Obteniendo datos del endpoint: {endpoint_name}\")\n",
    "    endpoint_response = endpoint_table_metadata.get_item(Key={'ENDPOINT_NAME': endpoint_name})\n",
    "    \n",
    "    if 'Item' not in endpoint_response:\n",
    "        raise CrawlerStageError(f\"Endpoint '{endpoint_name}' no encontrado en la tabla\")\n",
    "    \n",
    "    endpoint_data = endpoint_response['Item']\n",
    "    logger.info(f\"Datos del endpoint obtenidos exitosamente: {endpoint_data.get('BD_TYPE', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error obteniendo datos del endpoint: {str(e)}\")\n",
    "    raise CrawlerStageError(f\"No se pudieron obtener los datos del endpoint: {str(e)}\")\n",
    "\n",
    "data_catalog_database_name = f\"{args['TEAM']}_{args['DATA_SOURCE']}_{endpoint_name}_stage\".lower()\n",
    "data_catalog_crawler_name = data_catalog_database_name + \"_crawler\"\n",
    "\n",
    "logger.info(f\"Nombres generados - Database: {data_catalog_database_name}, Crawler: {data_catalog_crawler_name}\")\n",
    "\n",
    "@retry_on_failure()\n",
    "def create_database_data_catalog(database_data_catalog_name: str) -> bool:\n",
    "    \"\"\"Crea una base de datos en el cat√°logo de datos de Glue\"\"\"\n",
    "    logger.info(f\"Creando base de datos en cat√°logo: {database_data_catalog_name}\")\n",
    "    \n",
    "    try:\n",
    "        response = client_glue.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': database_data_catalog_name,\n",
    "                'Description': f'Database for {endpoint_name} stage data'\n",
    "            }\n",
    "        )\n",
    "        logger.info(f\"Base de datos '{database_data_catalog_name}' creada exitosamente\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'AlreadyExistsException':\n",
    "            logger.warning(f\"La base de datos '{database_data_catalog_name}' ya existe\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(f\"Error de cliente AWS creando base de datos: {error_code} - {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado creando base de datos: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "@retry_on_failure()\n",
    "def get_database_data_catalog(database_data_catalog_name: str) -> bool:\n",
    "    \"\"\"Verifica si existe una base de datos en el cat√°logo\"\"\"\n",
    "    logger.info(f\"Verificando existencia de base de datos: {database_data_catalog_name}\")\n",
    "    \n",
    "    try:\n",
    "        response = client_glue.get_database(Name=database_data_catalog_name)\n",
    "        logger.info(f\"Base de datos '{database_data_catalog_name}' encontrada\")\n",
    "        logger.debug(f\"Detalles de la base de datos: {response.get('Database', {}).get('Name', 'N/A')}\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'EntityNotFoundException':\n",
    "            logger.info(f\"Base de datos '{database_data_catalog_name}' no existe\")\n",
    "            return False\n",
    "        else:\n",
    "            logger.error(f\"Error verificando base de datos: {error_code} - {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado verificando base de datos: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "@retry_on_failure()\n",
    "def get_job_arn_role(job_name: str) -> Optional[str]:\n",
    "    \"\"\"Obtiene el ARN del rol asociado a un job de Glue\"\"\"\n",
    "    logger.info(f\"Obteniendo ARN del rol para el job: {job_name}\")\n",
    "    \n",
    "    try:\n",
    "        response = client_glue.get_job(JobName=job_name)\n",
    "        role_arn = response['Job']['Role']\n",
    "        logger.info(f\"ARN del rol obtenido: {role_arn}\")\n",
    "        return role_arn\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        logger.error(f\"Error obteniendo job role: {error_code} - {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado obteniendo job role: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@retry_on_failure()\n",
    "def grant_permissions_to_database_lakeformation(job_role_arn_name: str, database_data_catalog_name: str) -> None:\n",
    "    \"\"\"Otorga permisos a la base de datos en Lake Formation\"\"\"\n",
    "    logger.info(f\"Otorgando permisos de base de datos en Lake Formation\")\n",
    "    logger.info(f\"Principal: {job_role_arn_name}\")\n",
    "    logger.info(f\"Database: {database_data_catalog_name}\")\n",
    "    \n",
    "    try:\n",
    "        response = client_lakeformation.grant_permissions(\n",
    "            Principal={\n",
    "                'DataLakePrincipalIdentifier': job_role_arn_name\n",
    "            },\n",
    "            Resource={\n",
    "                'Database': {\n",
    "                    'Name': database_data_catalog_name\n",
    "                },\n",
    "            },\n",
    "            Permissions=['ALL'],\n",
    "            PermissionsWithGrantOption=['ALL']\n",
    "        )\n",
    "        logger.info(\"Permisos de base de datos otorgados exitosamente\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'AlreadyExistsException':\n",
    "            logger.warning(\"Los permisos ya existen para esta base de datos\")\n",
    "        else:\n",
    "            logger.error(f\"Error otorgando permisos de base de datos: {error_code} - {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado otorgando permisos de base de datos: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "@retry_on_failure()\n",
    "def create_lf_tag_if_not_exists(tag_key: str, tag_values: List[str]) -> bool:\n",
    "    \"\"\"Crea un LF-Tag si no existe\"\"\"\n",
    "    logger.info(f\"Verificando/creando LF-Tag: {tag_key} con valores: {tag_values}\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar si el tag existe\n",
    "        response = client_lakeformation.get_lf_tag(TagKey=tag_key)\n",
    "        existing_values = response['TagValues']\n",
    "        logger.info(f\"LF-Tag '{tag_key}' ya existe con valores: {existing_values}\")\n",
    "        \n",
    "        # Verificar si necesitamos agregar nuevos valores\n",
    "        missing_values = [val for val in tag_values if val not in existing_values]\n",
    "        if missing_values:\n",
    "            logger.info(f\"Actualizando LF-Tag con nuevos valores: {missing_values}\")\n",
    "            all_values = list(set(existing_values + tag_values))\n",
    "            client_lakeformation.update_lf_tag(\n",
    "                TagKey=tag_key,\n",
    "                TagValuesToAdd=missing_values\n",
    "            )\n",
    "            logger.info(f\"LF-Tag actualizado exitosamente\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'EntityNotFoundException':\n",
    "            # El tag no existe, crearlo\n",
    "            logger.info(f\"LF-Tag '{tag_key}' no existe, cre√°ndolo...\")\n",
    "            try:\n",
    "                response = client_lakeformation.create_lf_tag(\n",
    "                    TagKey=tag_key,\n",
    "                    TagValues=tag_values\n",
    "                )\n",
    "                logger.info(f\"LF-Tag '{tag_key}' creado exitosamente\")\n",
    "                return True\n",
    "            except Exception as create_error:\n",
    "                logger.error(f\"Error creando LF-Tag: {str(create_error)}\")\n",
    "                raise\n",
    "        else:\n",
    "            logger.error(f\"Error verificando LF-Tag: {error_code} - {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado con LF-Tag: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@retry_on_failure()\n",
    "def grant_permissions_lf_tag_lakeformation(job_role_arn_name: str) -> None:\n",
    "    \"\"\"Otorga permisos de LF-Tag en Lake Formation\"\"\"\n",
    "    logger.info(f\"Otorgando permisos de LF-Tag para el rol: {job_role_arn_name}\")\n",
    "    \n",
    "    try:\n",
    "        response = client_lakeformation.grant_permissions(\n",
    "            Principal={\n",
    "                'DataLakePrincipalIdentifier': job_role_arn_name\n",
    "            },\n",
    "            Resource={\n",
    "                'LFTag': {\n",
    "                    'TagKey': 'Level',\n",
    "                    'TagValues': ['Stage']\n",
    "                },\n",
    "            },\n",
    "            Permissions=['ASSOCIATE'],\n",
    "            PermissionsWithGrantOption=['ASSOCIATE']\n",
    "        )\n",
    "        logger.info(\"Permisos de LF-Tag otorgados exitosamente\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'AlreadyExistsException':\n",
    "            logger.warning(\"Los permisos de LF-Tag ya existen\")\n",
    "        else:\n",
    "            logger.error(f\"Error otorgando permisos de LF-Tag: {error_code} - {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado otorgando permisos de LF-Tag: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@retry_on_failure()\n",
    "def add_lf_tags_to_database_lakeformation(database_data_catalog_name: str) -> None:\n",
    "    \"\"\"Agrega LF-Tags a la base de datos\"\"\"\n",
    "    logger.info(f\"Agregando LF-Tags a la base de datos: {database_data_catalog_name}\")\n",
    "    \n",
    "    try:\n",
    "        response = client_lakeformation.add_lf_tags_to_resource(\n",
    "            Resource={\n",
    "                'Database': {\n",
    "                    'Name': database_data_catalog_name\n",
    "                },\n",
    "            },\n",
    "            LFTags=[\n",
    "                {\n",
    "                    'TagKey': 'Level',\n",
    "                    'TagValues': ['Stage']\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        logger.info(\"LF-Tags agregados exitosamente a la base de datos\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        logger.error(f\"Error agregando LF-Tags: {error_code} - {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado agregando LF-Tags: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def build_crawler_targets(total_list: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Construye la lista de targets para el crawler\"\"\"\n",
    "    logger.info(f\"Construyendo targets para {len(total_list)} tablas\")\n",
    "    tables = []\n",
    "    \n",
    "    for table in total_list:\n",
    "        try:\n",
    "            logger.debug(f\"Procesando tabla: {table}\")\n",
    "            \n",
    "            # Obtener datos de la tabla\n",
    "            table_response = config_table_metadata.get_item(Key={'TARGET_TABLE_NAME': table})\n",
    "            if 'Item' not in table_response:\n",
    "                logger.warning(f\"Tabla '{table}' no encontrada en configuraci√≥n, saltando...\")\n",
    "                continue\n",
    "                \n",
    "            table_data = table_response['Item']\n",
    "            \n",
    "            # Obtener datos del endpoint\n",
    "            endpoint_response = endpoint_table_metadata.get_item(Key={'ENDPOINT_NAME': table_data['ENDPOINT_NAME']})\n",
    "            if 'Item' not in endpoint_response:\n",
    "                logger.warning(f\"Endpoint '{table_data['ENDPOINT_NAME']}' no encontrado, saltando tabla '{table}'\")\n",
    "                continue\n",
    "                \n",
    "            endpoint_data = endpoint_response['Item']\n",
    "            \n",
    "            # Construir ruta S3\n",
    "            s3_path = f\"{s3_target}{args['TEAM']}/{args['DATA_SOURCE']}/{table_data['ENDPOINT_NAME']}/{table_data['STAGE_TABLE_NAME']}/\"\n",
    "            \n",
    "            data_source = {\n",
    "                'DeltaTables': [s3_path],\n",
    "                'ConnectionName': '',\n",
    "                'WriteManifest': True\n",
    "            }\n",
    "            \n",
    "            tables.append(data_source)\n",
    "            logger.debug(f\"Target agregado para tabla '{table}': {s3_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error procesando tabla '{table}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Se construyeron {len(tables)} targets exitosamente\")\n",
    "    return tables\n",
    "\n",
    "@retry_on_failure()\n",
    "def create_crawler(total_list: List[str]) -> bool:\n",
    "    \"\"\"Crea un nuevo crawler en Glue\"\"\"\n",
    "    logger.info(f\"Creando crawler: {data_catalog_crawler_name}\")\n",
    "    \n",
    "    try:\n",
    "        tables = build_crawler_targets(total_list)\n",
    "        \n",
    "        if not tables:\n",
    "            logger.warning(\"No se encontraron tablas v√°lidas para el crawler\")\n",
    "            return False\n",
    "        \n",
    "        response = client_glue.create_crawler(\n",
    "            Name=data_catalog_crawler_name,\n",
    "            Role=arn_role_crawler,\n",
    "            DatabaseName=data_catalog_database_name,\n",
    "            Description=f'Crawler for {endpoint_name} stage tables',\n",
    "            Targets={\n",
    "                'DeltaTargets': tables\n",
    "            },\n",
    "            Configuration=json.dumps({\n",
    "                \"Version\": 1.0,\n",
    "                \"CrawlerOutput\": {\n",
    "                    \"Partitions\": {\"AddOrUpdateBehavior\": \"InheritFromTable\"}\n",
    "                }\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Crawler '{data_catalog_crawler_name}' creado exitosamente\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'AlreadyExistsException':\n",
    "            logger.warning(f\"El crawler '{data_catalog_crawler_name}' ya existe\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(f\"Error creando crawler: {error_code} - {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado creando crawler: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "@retry_on_failure()\n",
    "def edit_crawler(total_list: List[str]) -> bool:\n",
    "    \"\"\"Actualiza la configuraci√≥n de un crawler existente\"\"\"\n",
    "    logger.info(f\"Actualizando crawler: {data_catalog_crawler_name}\")\n",
    "    \n",
    "    try:\n",
    "        tables = build_crawler_targets(total_list)\n",
    "        \n",
    "        if not tables:\n",
    "            logger.warning(\"No se encontraron tablas v√°lidas para actualizar el crawler\")\n",
    "            return False\n",
    "        \n",
    "        response = client_glue.update_crawler(\n",
    "            Name=data_catalog_crawler_name,\n",
    "            Role=arn_role_crawler,\n",
    "            DatabaseName=data_catalog_database_name,\n",
    "            Description=f'Updated crawler for {endpoint_name} stage tables',\n",
    "            Targets={\n",
    "                'DeltaTargets': tables\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Crawler '{data_catalog_crawler_name}' actualizado exitosamente\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        logger.error(f\"Error actualizando crawler: {error_code} - {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado actualizando crawler: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@retry_on_failure()\n",
    "def get_crawler(crawler_name: str) -> bool:\n",
    "    \"\"\"Verifica si existe un crawler\"\"\"\n",
    "    logger.info(f\"Verificando existencia del crawler: {crawler_name}\")\n",
    "    \n",
    "    try:\n",
    "        response = client_glue.get_crawler(Name=crawler_name)\n",
    "        crawler_state = response['Crawler']['State']\n",
    "        logger.info(f\"Crawler '{crawler_name}' encontrado, estado: {crawler_state}\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'EntityNotFoundException':\n",
    "            logger.info(f\"Crawler '{crawler_name}' no existe\")\n",
    "            return False\n",
    "        else:\n",
    "            logger.error(f\"Error verificando crawler: {error_code} - {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado verificando crawler: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "@retry_on_failure()\n",
    "def start_crawler(crawler_name: str) -> bool:\n",
    "    \"\"\"Inicia la ejecuci√≥n de un crawler\"\"\"\n",
    "    logger.info(f\"Iniciando crawler: {crawler_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar estado actual del crawler\n",
    "        response = client_glue.get_crawler(Name=crawler_name)\n",
    "        current_state = response['Crawler']['State']\n",
    "        \n",
    "        if current_state == 'RUNNING':\n",
    "            logger.info(f\"El crawler '{crawler_name}' ya est√° ejecut√°ndose\")\n",
    "            return True\n",
    "        elif current_state in ['STOPPING', 'READY']:\n",
    "            logger.info(f\"Estado del crawler: {current_state}, procediendo a iniciar...\")\n",
    "        else:\n",
    "            logger.warning(f\"Estado inesperado del crawler: {current_state}\")\n",
    "        \n",
    "        # Iniciar crawler\n",
    "        start_response = client_glue.start_crawler(Name=crawler_name)\n",
    "        logger.info(f\"Crawler '{crawler_name}' iniciado exitosamente\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'CrawlerRunningException':\n",
    "            logger.info(f\"El crawler '{crawler_name}' ya est√° ejecut√°ndose\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(f\"Error iniciando crawler: {error_code} - {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado iniciando crawler: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@retry_on_failure()\n",
    "def update_attribute_value_dynamodb(row_key_field_name: str, row_key: str, \n",
    "                                   attribute_name: str, attribute_value: Any, \n",
    "                                   table_name: str) -> bool:\n",
    "    \"\"\"Actualiza un atributo en DynamoDB\"\"\"\n",
    "    logger.info(f'Actualizando DynamoDB - Tabla: {table_name}, Key: {row_key}, Atributo: {attribute_name}')\n",
    "    \n",
    "    try:\n",
    "        dynamo_table = dynamodb.Table(table_name)\n",
    "        response = dynamo_table.update_item(\n",
    "            Key={row_key_field_name: row_key},\n",
    "            AttributeUpdates={\n",
    "                attribute_name: {\n",
    "                    'Value': attribute_value,\n",
    "                    'Action': 'PUT'\n",
    "                }\n",
    "            },\n",
    "            ReturnValues='UPDATED_NEW'\n",
    "        )\n",
    "        \n",
    "        logger.info(f'Atributo actualizado exitosamente en DynamoDB: {row_key}')\n",
    "        logger.debug(f'Respuesta de DynamoDB: {response.get(\"Attributes\", {})}')\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        logger.error(f\"Error de cliente DynamoDB: {error_code} - {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inesperado actualizando DynamoDB: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_dynamo_crawler_status_for_endpoint(endpoint_name: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Obtiene el estado del crawler para las tablas de un endpoint\"\"\"\n",
    "    logger.info(f\"Obteniendo estado del crawler para endpoint: {endpoint_name}\")\n",
    "    \n",
    "    total_list = []\n",
    "    empty_table = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    try:\n",
    "        # Escanear la tabla de configuraci√≥n\n",
    "        logger.info(\"Escaneando tabla de configuraci√≥n...\")\n",
    "        response = config_table_metadata.scan()\n",
    "        items = response.get('Items', [])\n",
    "        logger.info(f\"Se encontraron {len(items)} elementos en la tabla de configuraci√≥n\")\n",
    "        \n",
    "        for stage_output in items:\n",
    "            try:\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Validar que el elemento tenga la estructura esperada\n",
    "                if 'ENDPOINT_NAME' not in stage_output:\n",
    "                    logger.debug(f\"Elemento sin ENDPOINT encontrado, saltando...\")\n",
    "                    continue\n",
    "                \n",
    "                if stage_output['ENDPOINT_NAME'] == endpoint_name:\n",
    "                    table_name = stage_output.get('TARGET_TABLE_NAME')\n",
    "                    if not table_name:\n",
    "                        logger.warning(\"Elemento sin TARGET_TABLE_NAME encontrado\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Verificar estado del crawler\n",
    "                    has_crawler = stage_output.get('CRAWLER', False)\n",
    "                    \n",
    "                    if not has_crawler:\n",
    "                        logger.info(f\"Tabla '{table_name}' necesita ser agregada al crawler\")\n",
    "                        empty_table.append(table_name)\n",
    "                    \n",
    "                    total_list.append(table_name)\n",
    "                    logger.debug(f\"Tabla procesada: {table_name}, tiene crawler: {has_crawler}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                table_name = stage_output.get('TARGET_TABLE_NAME', 'UNKNOWN')\n",
    "                logger.error(f\"Error procesando tabla '{table_name}': {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Procesamiento completado - Total: {len(total_list)}, Sin crawler: {len(empty_table)}, Errores: {error_count}\")\n",
    "        \n",
    "        # Actualizar tablas que necesitan crawler\n",
    "        updated_count = 0\n",
    "        for table in empty_table:\n",
    "            try:\n",
    "                if update_attribute_value_dynamodb('TARGET_TABLE_NAME', table, 'CRAWLER', True, dynamo_config_table):\n",
    "                    updated_count += 1\n",
    "                    logger.info(f\"Tabla '{table}' marcada como agregada al crawler\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error actualizando estado de crawler para tabla '{table}': {str(e)}\")\n",
    "        \n",
    "        logger.info(f\"Se actualizaron {updated_count} de {len(empty_table)} tablas en DynamoDB\")\n",
    "        \n",
    "        return total_list, empty_table\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cr√≠tico obteniendo estado del crawler: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise CrawlerStageError(f\"No se pudo obtener el estado del crawler: {str(e)}\")\n",
    "\n",
    "# Funci√≥n principal con manejo robusto de errores\n",
    "def main():\n",
    "   \"\"\"Funci√≥n principal del proceso\"\"\"\n",
    "   start_time = time.time()\n",
    "   logger.info(\"=\"*60)\n",
    "   logger.info(\"INICIANDO PROCESO CRAWLER STAGE\")\n",
    "   logger.info(\"=\"*60)\n",
    "   \n",
    "   try:\n",
    "       # Obtener estado de las tablas\n",
    "       logger.info(\"Paso 1: Obteniendo estado de las tablas...\")\n",
    "       total_list, empty_table = get_dynamo_crawler_status_for_endpoint(endpoint_name)\n",
    "       \n",
    "       if not total_list:\n",
    "           logger.warning(f\"No se encontraron tablas para el endpoint '{endpoint_name}'\")\n",
    "           logger.info(\"Proceso finalizado - No hay tablas para procesar\")\n",
    "           return\n",
    "       \n",
    "       logger.info(f\"Tablas encontradas: {len(total_list)}, Nuevas tablas: {len(empty_table)}\")\n",
    "       \n",
    "       # Verificar si el crawler existe\n",
    "       logger.info(\"Paso 2: Verificando existencia del crawler...\")\n",
    "       crawler_exists = get_crawler(data_catalog_crawler_name)\n",
    "       \n",
    "       if crawler_exists:\n",
    "           logger.info(\"El crawler ya existe\")\n",
    "           \n",
    "           # Si hay nuevas tablas, actualizar el crawler\n",
    "           if len(empty_table) > 0:\n",
    "               logger.info(f\"Actualizando crawler con {len(empty_table)} nuevas tablas...\")\n",
    "               if edit_crawler(total_list):\n",
    "                   logger.info(\"Crawler actualizado exitosamente\")\n",
    "               else:\n",
    "                   logger.error(\"Error actualizando el crawler\")\n",
    "                   return\n",
    "           \n",
    "           logger.info(\"Paso 3: Iniciando crawler existente...\")\n",
    "           if start_crawler(data_catalog_crawler_name):\n",
    "               logger.info(\"Crawler iniciado exitosamente\")\n",
    "           else:\n",
    "               logger.error(\"Error iniciando el crawler\")\n",
    "               return\n",
    "               \n",
    "       else:\n",
    "           logger.info(\"El crawler no existe, procediendo a crearlo...\")\n",
    "           \n",
    "           # Verificar si existe la base de datos\n",
    "           logger.info(\"Paso 3: Verificando base de datos del cat√°logo...\")\n",
    "           database_exists = get_database_data_catalog(data_catalog_database_name)\n",
    "           \n",
    "           if database_exists:\n",
    "               logger.info(\"La base de datos ya existe\")\n",
    "               \n",
    "               # Crear el crawler\n",
    "               logger.info(\"Paso 4: Creando nuevo crawler...\")\n",
    "               if create_crawler(total_list):\n",
    "                   logger.info(\"Crawler creado exitosamente\")\n",
    "                   \n",
    "                   logger.info(\"Paso 5: Iniciando nuevo crawler...\")\n",
    "                   if start_crawler(data_catalog_crawler_name):\n",
    "                       logger.info(\"Crawler iniciado exitosamente\")\n",
    "                   else:\n",
    "                       logger.error(\"Error iniciando el nuevo crawler\")\n",
    "                       return\n",
    "               else:\n",
    "                   logger.error(\"Error creando el crawler\")\n",
    "                   return\n",
    "                   \n",
    "           else:\n",
    "               logger.info(\"La base de datos no existe, creando infraestructura completa...\")\n",
    "               \n",
    "               # Configurar permisos y crear base de datos\n",
    "               job_role_arn_name = arn_role_crawler\n",
    "               logger.info(f\"Usando ARN del rol: {job_role_arn_name}\")\n",
    "               \n",
    "               # AGREGAR ESTO ANTES de otorgar permisos:\n",
    "               logger.info(\"Paso 3.5: Creando/verificando LF-Tag...\")\n",
    "               try:\n",
    "                   create_lf_tag_if_not_exists('Level', ['Stage'])\n",
    "               except Exception as e:\n",
    "                   logger.warning(f\"Error creando LF-Tag (continuando): {str(e)}\")\n",
    "               \n",
    "               logger.info(\"Paso 4: Otorgando permisos de LF-Tag...\")\n",
    "               try:\n",
    "                   grant_permissions_lf_tag_lakeformation(job_role_arn_name)\n",
    "               except Exception as e:\n",
    "                   logger.warning(f\"Error otorgando permisos LF-Tag (continuando): {str(e)}\")\n",
    "               \n",
    "               logger.info(\"Paso 5: Creando base de datos del cat√°logo...\")\n",
    "               if create_database_data_catalog(data_catalog_database_name):\n",
    "                   logger.info(\"Base de datos creada exitosamente\")\n",
    "               else:\n",
    "                   logger.error(\"Error creando la base de datos\")\n",
    "                   return\n",
    "               \n",
    "               logger.info(\"Paso 6: Agregando LF-Tags a la base de datos...\")\n",
    "               try:\n",
    "                   add_lf_tags_to_database_lakeformation(data_catalog_database_name)\n",
    "               except Exception as e:\n",
    "                   logger.warning(f\"Error agregando LF-Tags (continuando): {str(e)}\")\n",
    "               \n",
    "               logger.info(\"Paso 7: Otorgando permisos de base de datos...\")\n",
    "               try:\n",
    "                   grant_permissions_to_database_lakeformation(job_role_arn_name, data_catalog_database_name)\n",
    "               except Exception as e:\n",
    "                   logger.warning(f\"Error otorgando permisos de base de datos (continuando): {str(e)}\")\n",
    "               \n",
    "               logger.info(\"Paso 8: Creando crawler...\")\n",
    "               if create_crawler(total_list):\n",
    "                   logger.info(\"Crawler creado exitosamente\")\n",
    "                   \n",
    "                   logger.info(\"Paso 9: Iniciando crawler...\")\n",
    "                   if start_crawler(data_catalog_crawler_name):\n",
    "                       logger.info(\"Crawler iniciado exitosamente\")\n",
    "                   else:\n",
    "                       logger.error(\"Error iniciando el crawler\")\n",
    "                       return\n",
    "               else:\n",
    "                   logger.error(\"Error creando el crawler\")\n",
    "                   return\n",
    "       \n",
    "       # Calcular tiempo de ejecuci√≥n\n",
    "       execution_time = time.time() - start_time\n",
    "       logger.info(\"=\"*60)\n",
    "       logger.info(\"PROCESO COMPLETADO EXITOSAMENTE\")\n",
    "       logger.info(f\"Tiempo de ejecuci√≥n: {execution_time:.2f} segundos\")\n",
    "       logger.info(f\"Crawler: {data_catalog_crawler_name}\")\n",
    "       logger.info(f\"Base de datos: {data_catalog_database_name}\")\n",
    "       logger.info(f\"Tablas procesadas: {len(total_list)}\")\n",
    "       logger.info(f\"Nuevas tablas agregadas: {len(empty_table)}\")\n",
    "       logger.info(\"=\"*60)\n",
    "       \n",
    "   except CrawlerStageError as e:\n",
    "       logger.error(f\"Error espec√≠fico del CrawlerStage: {str(e)}\")\n",
    "       logger.error(\"El proceso se detuvo debido a un error controlado\")\n",
    "       sys.exit(1)\n",
    "       \n",
    "   except ClientError as e:\n",
    "       error_code = e.response['Error']['Code']\n",
    "       error_message = e.response['Error']['Message']\n",
    "       logger.error(f\"Error de cliente AWS: {error_code} - {error_message}\")\n",
    "       logger.error(\"El proceso se detuvo debido a un error de AWS\")\n",
    "       sys.exit(1)\n",
    "       \n",
    "   except Exception as e:\n",
    "       execution_time = time.time() - start_time\n",
    "       logger.critical(f\"Error cr√≠tico no manejado: {str(e)}\")\n",
    "       logger.critical(f\"Traceback completo: {traceback.format_exc()}\")\n",
    "       logger.critical(f\"Tiempo transcurrido antes del error: {execution_time:.2f} segundos\")\n",
    "       logger.critical(\"=\"*60)\n",
    "       logger.critical(\"PROCESO TERMINADO CON ERROR CR√çTICO\")\n",
    "       logger.critical(\"=\"*60)\n",
    "       sys.exit(1)\n",
    "\n",
    "def health_check() -> bool:\n",
    "   \"\"\"Realiza verificaciones de salud del sistema antes de ejecutar el proceso principal\"\"\"\n",
    "   logger.info(\"Ejecutando verificaciones de salud...\")\n",
    "   \n",
    "   health_checks = []\n",
    "   \n",
    "   try:\n",
    "       # 1. Verificar conectividad de Glue\n",
    "       logger.debug(\"Verificando conectividad con AWS Glue...\")\n",
    "       try:\n",
    "           # CORREGIDO: Usar get_databases en lugar de list_databases\n",
    "           glue_response = client_glue.get_databases(MaxResults=1)\n",
    "           health_checks.append((\"AWS Glue\", True, \"Conectividad OK\"))\n",
    "           logger.debug(\"‚úÖ AWS Glue - Conectividad verificada\")\n",
    "       except Exception as e:\n",
    "           health_checks.append((\"AWS Glue\", False, f\"Error: {str(e)}\"))\n",
    "           logger.error(f\"‚ùå AWS Glue - Error de conectividad: {str(e)}\")\n",
    "       \n",
    "       # 2. Verificar conectividad de Lake Formation (CORREGIDO)\n",
    "       logger.debug(\"Verificando conectividad con AWS Lake Formation...\")\n",
    "       try:\n",
    "           # En lugar de hacer una llamada espec√≠fica, verificamos que el cliente se inicialice correctamente\n",
    "           # y que tengamos permisos b√°sicos\n",
    "           lf_response = client_lakeformation.list_permissions(MaxResults=1)\n",
    "           health_checks.append((\"AWS Lake Formation\", True, \"Conectividad OK\"))\n",
    "           logger.debug(\"‚úÖ AWS Lake Formation - Conectividad verificada\")\n",
    "       except ClientError as e:\n",
    "           error_code = e.response['Error']['Code']\n",
    "           # Algunos errores son esperados dependiendo de los permisos\n",
    "           if error_code in ['AccessDeniedException']:\n",
    "               health_checks.append((\"AWS Lake Formation\", True, \"Cliente inicializado (permisos limitados)\"))\n",
    "               logger.debug(\"‚úÖ AWS Lake Formation - Cliente funcional con permisos limitados\")\n",
    "           else:\n",
    "               health_checks.append((\"AWS Lake Formation\", False, f\"Error: {error_code}\"))\n",
    "               logger.error(f\"‚ùå AWS Lake Formation - Error: {error_code}\")\n",
    "       except Exception as e:\n",
    "           health_checks.append((\"AWS Lake Formation\", False, f\"Error: {str(e)}\"))\n",
    "           logger.error(f\"‚ùå AWS Lake Formation - Error de conectividad: {str(e)}\")\n",
    "       \n",
    "       # 3. Verificar acceso a tablas DynamoDB (CORREGIDO)\n",
    "       logger.debug(\"Verificando acceso a tablas DynamoDB...\")\n",
    "       try:\n",
    "           # CORREGIDO: Usar el cliente DynamoDB para describe_table, no el resource\n",
    "           config_response = dynamodb_client.describe_table(TableName=dynamo_config_table)\n",
    "           table_status = config_response['Table']['TableStatus']\n",
    "           if table_status == 'ACTIVE':\n",
    "               health_checks.append((\"DynamoDB Config Table\", True, f\"Estado: {table_status}\"))\n",
    "               logger.debug(f\"‚úÖ Tabla de configuraci√≥n DynamoDB - Estado: {table_status}\")\n",
    "           else:\n",
    "               health_checks.append((\"DynamoDB Config Table\", False, f\"Estado no activo: {table_status}\"))\n",
    "               logger.warning(f\"‚ö†Ô∏è Tabla de configuraci√≥n DynamoDB - Estado: {table_status}\")\n",
    "       except Exception as e:\n",
    "           health_checks.append((\"DynamoDB Config Table\", False, f\"Error: {str(e)}\"))\n",
    "           logger.error(f\"‚ùå Tabla de configuraci√≥n DynamoDB - Error: {str(e)}\")\n",
    "       \n",
    "       try:\n",
    "           # CORREGIDO: Usar el cliente DynamoDB para describe_table, no el resource\n",
    "           endpoint_response = dynamodb_client.describe_table(TableName=dynamo_endpoint_table)\n",
    "           table_status = endpoint_response['Table']['TableStatus']\n",
    "           if table_status == 'ACTIVE':\n",
    "               health_checks.append((\"DynamoDB Endpoint Table\", True, f\"Estado: {table_status}\"))\n",
    "               logger.debug(f\"‚úÖ Tabla de endpoints DynamoDB - Estado: {table_status}\")\n",
    "           else:\n",
    "               health_checks.append((\"DynamoDB Endpoint Table\", False, f\"Estado no activo: {table_status}\"))\n",
    "               logger.warning(f\"‚ö†Ô∏è Tabla de endpoints DynamoDB - Estado: {table_status}\")\n",
    "       except Exception as e:\n",
    "           health_checks.append((\"DynamoDB Endpoint Table\", False, f\"Error: {str(e)}\"))\n",
    "           logger.error(f\"‚ùå Tabla de endpoints DynamoDB - Error: {str(e)}\")\n",
    "       \n",
    "       # 4. Verificar permisos del rol\n",
    "       logger.debug(\"Verificando permisos del rol...\")\n",
    "       try:\n",
    "           # Test b√°sico de permisos intentando describir el rol\n",
    "           sts_client = boto3.client('sts')\n",
    "           identity = sts_client.get_caller_identity()\n",
    "           health_checks.append((\"IAM Permissions\", True, f\"Principal: {identity.get('Arn', 'Unknown')}\"))\n",
    "           logger.debug(f\"‚úÖ Permisos IAM - Principal verificado: {identity.get('Arn', 'Unknown')}\")\n",
    "       except Exception as e:\n",
    "           health_checks.append((\"IAM Permissions\", False, f\"Error: {str(e)}\"))\n",
    "           logger.error(f\"‚ùå Permisos IAM - Error: {str(e)}\")\n",
    "       \n",
    "       # 5. Verificar acceso a S3\n",
    "       logger.debug(\"Verificando acceso a S3...\")\n",
    "       try:\n",
    "           s3_client = boto3.client('s3')\n",
    "           # Extraer bucket name del S3 target\n",
    "           bucket_name = s3_target.replace('s3://', '').split('/')[0]\n",
    "           s3_response = s3_client.head_bucket(Bucket=bucket_name)\n",
    "           health_checks.append((\"S3 Access\", True, f\"Bucket accesible: {bucket_name}\"))\n",
    "           logger.debug(f\"‚úÖ S3 - Bucket accesible: {bucket_name}\")\n",
    "       except Exception as e:\n",
    "           health_checks.append((\"S3 Access\", False, f\"Error: {str(e)}\"))\n",
    "           logger.error(f\"‚ùå S3 - Error de acceso: {str(e)}\")\n",
    "       \n",
    "       # 6. Verificar conectividad b√°sica de endpoint de datos (opcional pero √∫til)\n",
    "       logger.debug(\"Verificando datos del endpoint...\")\n",
    "       try:\n",
    "           # Verificar que podemos leer los datos del endpoint\n",
    "           endpoint_response = endpoint_table_metadata.get_item(Key={'ENDPOINT_NAME': endpoint_name})\n",
    "           if 'Item' in endpoint_response:\n",
    "               health_checks.append((\"Endpoint Data\", True, f\"Endpoint '{endpoint_name}' encontrado\"))\n",
    "               logger.debug(f\"‚úÖ Datos del endpoint - Endpoint '{endpoint_name}' accesible\")\n",
    "           else:\n",
    "               health_checks.append((\"Endpoint Data\", False, f\"Endpoint '{endpoint_name}' no encontrado\"))\n",
    "               logger.error(f\"‚ùå Datos del endpoint - Endpoint '{endpoint_name}' no encontrado\")\n",
    "       except Exception as e:\n",
    "           health_checks.append((\"Endpoint Data\", False, f\"Error: {str(e)}\"))\n",
    "           logger.error(f\"‚ùå Datos del endpoint - Error: {str(e)}\")\n",
    "       \n",
    "       # Resumen de verificaciones de salud\n",
    "       logger.info(\"Resumen de verificaciones de salud:\")\n",
    "       logger.info(\"-\" * 60)\n",
    "       \n",
    "       failed_checks = 0\n",
    "       critical_failures = 0\n",
    "       \n",
    "       for service, status, message in health_checks:\n",
    "           status_symbol = \"‚úÖ\" if status else \"‚ùå\"\n",
    "           logger.info(f\"{status_symbol} {service:<25} - {message}\")\n",
    "           if not status:\n",
    "               failed_checks += 1\n",
    "               # Marcar fallas cr√≠ticas que impedir√≠an la ejecuci√≥n\n",
    "               if service in [\"AWS Glue\", \"DynamoDB Config Table\", \"DynamoDB Endpoint Table\", \"Endpoint Data\"]:\n",
    "                   critical_failures += 1\n",
    "       \n",
    "       logger.info(\"-\" * 60)\n",
    "       \n",
    "       if failed_checks == 0:\n",
    "           logger.info(\"üéâ Todas las verificaciones de salud pasaron exitosamente\")\n",
    "           return True\n",
    "       elif critical_failures == 0:\n",
    "           logger.warning(f\"‚ö†Ô∏è {failed_checks} verificaciones fallaron, pero ninguna es cr√≠tica\")\n",
    "           logger.warning(\"Continuando con el proceso...\")\n",
    "           return True\n",
    "       else:\n",
    "           logger.error(f\"üö® {critical_failures} verificaciones cr√≠ticas fallaron de {failed_checks} totales\")\n",
    "           logger.error(\"No se puede continuar con el proceso\")\n",
    "           return False\n",
    "           \n",
    "   except Exception as e:\n",
    "       logger.error(f\"Error durante las verificaciones de salud: {str(e)}\")\n",
    "       logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "       return False\n",
    "\n",
    "# Funci√≥n de monitoreo del progreso del crawler\n",
    "@retry_on_failure()\n",
    "def monitor_crawler_progress(crawler_name: str, timeout_minutes: int = 30) -> bool:\n",
    "   \"\"\"Monitorea el progreso del crawler hasta su finalizaci√≥n\"\"\"\n",
    "   logger.info(f\"Iniciando monitoreo del crawler: {crawler_name}\")\n",
    "   logger.info(f\"Timeout configurado: {timeout_minutes} minutos\")\n",
    "   \n",
    "   start_time = time.time()\n",
    "   timeout_seconds = timeout_minutes * 60\n",
    "   check_interval = 30  # Verificar cada 30 segundos\n",
    "   \n",
    "   try:\n",
    "       while True:\n",
    "           elapsed_time = time.time() - start_time\n",
    "           \n",
    "           # Verificar timeout\n",
    "           if elapsed_time > timeout_seconds:\n",
    "               logger.warning(f\"Timeout alcanzado ({timeout_minutes} minutos) - Deteniendo monitoreo\")\n",
    "               return False\n",
    "           \n",
    "           # Obtener estado del crawler\n",
    "           response = client_glue.get_crawler(Name=crawler_name)\n",
    "           crawler_state = response['Crawler']['State']\n",
    "           \n",
    "           # Log del estado actual\n",
    "           minutes_elapsed = elapsed_time / 60\n",
    "           logger.info(f\"Estado del crawler [{minutes_elapsed:.1f}min]: {crawler_state}\")\n",
    "           \n",
    "           # Verificar estados finales\n",
    "           if crawler_state == 'READY':\n",
    "               # Obtener estad√≠sticas de la √∫ltima ejecuci√≥n\n",
    "               last_crawl = response['Crawler'].get('LastCrawl', {})\n",
    "               if last_crawl:\n",
    "                   status = last_crawl.get('Status', 'UNKNOWN')\n",
    "                   tables_created = last_crawl.get('TablesCreated', 0)\n",
    "                   tables_updated = last_crawl.get('TablesUpdated', 0)\n",
    "                   tables_deleted = last_crawl.get('TablesDeleted', 0)\n",
    "                   \n",
    "                   logger.info(f\"Crawler completado exitosamente:\")\n",
    "                   logger.info(f\"  - Estado final: {status}\")\n",
    "                   logger.info(f\"  - Tablas creadas: {tables_created}\")\n",
    "                   logger.info(f\"  - Tablas actualizadas: {tables_updated}\")\n",
    "                   logger.info(f\"  - Tablas eliminadas: {tables_deleted}\")\n",
    "                   logger.info(f\"  - Tiempo total: {minutes_elapsed:.1f} minutos\")\n",
    "               \n",
    "               return True\n",
    "               \n",
    "           elif crawler_state in ['STOPPING', 'STOPPED']:\n",
    "               logger.warning(f\"Crawler detenido inesperadamente - Estado: {crawler_state}\")\n",
    "               \n",
    "               # Intentar obtener informaci√≥n del error si est√° disponible\n",
    "               last_crawl = response['Crawler'].get('LastCrawl', {})\n",
    "               if last_crawl and 'ErrorMessage' in last_crawl:\n",
    "                   logger.error(f\"Error del crawler: {last_crawl['ErrorMessage']}\")\n",
    "               \n",
    "               return False\n",
    "               \n",
    "           elif crawler_state == 'RUNNING':\n",
    "               # Obtener progreso si est√° disponible\n",
    "               last_crawl = response['Crawler'].get('LastCrawl', {})\n",
    "               if last_crawl and 'TablesCreated' in last_crawl:\n",
    "                   logger.debug(f\"Progreso: {last_crawl.get('TablesCreated', 0)} tablas procesadas\")\n",
    "           \n",
    "           # Esperar antes de la siguiente verificaci√≥n\n",
    "           time.sleep(check_interval)\n",
    "           \n",
    "   except ClientError as e:\n",
    "       error_code = e.response['Error']['Code']\n",
    "       logger.error(f\"Error monitoreando crawler: {error_code} - {str(e)}\")\n",
    "       raise\n",
    "   except Exception as e:\n",
    "       logger.error(f\"Error inesperado monitoreando crawler: {str(e)}\")\n",
    "       raise\n",
    "\n",
    "# Funci√≥n para limpiar recursos en caso de error\n",
    "def cleanup_on_error(crawler_name: str, database_name: str):\n",
    "   \"\"\"Limpia recursos parcialmente creados en caso de error\"\"\"\n",
    "   logger.info(\"Iniciando limpieza de recursos debido a error...\")\n",
    "   \n",
    "   try:\n",
    "       # Intentar detener el crawler si est√° corriendo\n",
    "       try:\n",
    "           crawler_response = client_glue.get_crawler(Name=crawler_name)\n",
    "           if crawler_response['Crawler']['State'] == 'RUNNING':\n",
    "               logger.info(f\"Deteniendo crawler en ejecuci√≥n: {crawler_name}\")\n",
    "               client_glue.stop_crawler(Name=crawler_name)\n",
    "               \n",
    "               # Esperar a que se detenga\n",
    "               max_wait = 60  # 1 minuto\n",
    "               wait_time = 0\n",
    "               while wait_time < max_wait:\n",
    "                   time.sleep(5)\n",
    "                   wait_time += 5\n",
    "                   state_response = client_glue.get_crawler(Name=crawler_name)\n",
    "                   if state_response['Crawler']['State'] != 'RUNNING':\n",
    "                       logger.info(\"Crawler detenido exitosamente\")\n",
    "                       break\n",
    "       except ClientError as e:\n",
    "           if e.response['Error']['Code'] != 'EntityNotFoundException':\n",
    "               logger.warning(f\"Error deteniendo crawler: {str(e)}\")\n",
    "       \n",
    "       logger.info(\"Limpieza completada\")\n",
    "       \n",
    "   except Exception as e:\n",
    "       logger.error(f\"Error durante la limpieza: {str(e)}\")\n",
    "\n",
    "# Punto de entrada principal\n",
    "if __name__ == \"__main__\":\n",
    "   try:\n",
    "       # Ejecutar verificaciones de salud primero\n",
    "       logger.info(\"Iniciando verificaciones de salud del sistema...\")\n",
    "       if not health_check():\n",
    "           logger.error(\"Las verificaciones de salud fallaron, abortando proceso\")\n",
    "           sys.exit(1)\n",
    "       \n",
    "       # Ejecutar proceso principal\n",
    "       main()\n",
    "       \n",
    "   except KeyboardInterrupt:\n",
    "       logger.warning(\"Proceso interrumpido por el usuario\")\n",
    "       cleanup_on_error(data_catalog_crawler_name, data_catalog_database_name)\n",
    "       sys.exit(1)\n",
    "   except Exception as e:\n",
    "       logger.critical(f\"Error cr√≠tico no capturado: {str(e)}\")\n",
    "       logger.critical(f\"Traceback: {traceback.format_exc()}\")\n",
    "       cleanup_on_error(data_catalog_crawler_name, data_catalog_database_name)\n",
    "       sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
